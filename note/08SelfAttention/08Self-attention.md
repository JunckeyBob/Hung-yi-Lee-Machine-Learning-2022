# Self-attention
## Complicate Input
So far, the input used by the model can be viewed as a vector, but when encountering more complicate inputs, such as inputting a sequence or inputting vectors of different lengths each time. 
 - Sentence: A vector of variable length
 - Sound: A sequence of vectors
 - Graph: A sequence of vectors
 - Drug Discovery: A graph

We can classify this type of problem based on the output. 
 -  N to N: Each vector has a label. (Sequence Labeling)
 -  N to 1: The whole sequence has a label. 
 -  N to many: Model decides the number of labels by itself. (Seq2Seq)

## Sequence Labelling
input number = output number
### Window Slide
Former models are only trained on one word and cannot judge nouns and verbs. The relationship between words should be considered, so that FC can consider the relationship between context and use a window to cover part of the sequence. 

But window slide has limitation. It's hard to decide the length of window and a large window leads to dramadically increased training data and overfitting. 

### Attention is all you need!
![](./figures/self-attention.png)

Self-attention will input the information of an entire sequence and output the same number of results, and it will consider an entire sequence during training.

#### Caculating Relevance
 - Dot-product
  $\alpha_{i, j} = W^q a_i \cdot W^k a_j$
 - Additive
  $\alpha_{i, j} = W tanh(W^q a_i + W^k a_j)$

![](./figures/relavance.png)

After caculating relevance, we use **Softmax** or **ReLU** to deal with the relevance. This process can increase flexiblity of relevance caculation. 

#### Extract Information
![](./figures/extract.png)
$$
b^i = \sum_{j}\alpha'_{i, j}v^j
$$

#### Cookbook of Self-attention
 - Preprocessing
  $$
  \left\{
    \begin{aligned}
    Q = W^q In\\
    K = W^k In\\
    V = W^v In\\
    \end{aligned}
  \right.
  $$
 - Caculating relevance
  $$
  A' \overset{Softmax/ReLU}{\Leftarrow} A = K^TQ
  $$
 - Extract information
  $$
  Out = VA'
  $$

The parameters of Self-attention is $W^q, W^k, W^v(O(n^2))$

#### Multi-head Self-attention
![](./figures/multi-head.png)

Get different types of relavance. And then combine outputs generated by different parameters to get the final output. 
$$
Out = W^0 
\begin{bmatrix}
Out^0 \\
Out^1 \\
\vdots \\
Out^n \\
\end{bmatrix}
$$

#### Positional Encoding
![](./figures/positional.png)
 - No position information in self-attention
 - Each position has a unique positional vector $e^i$
 - hand-crafted
 - learned from data

#### CNN v.s. Self-attention
![](./figures/relaton.png)
 - CNN is simplified self-attention
  CNN is self-attention that can only attends in a receptive field
 - Self-attention is the complex version of CNN
  Self-attention is CNN with learnable receptive field

![](./figures/accurance.png)
 - CNN is good for less data
 - Self-attention is good for more data

## XXformer
### Introduction
Traditional self-attention needs caculating $A'$, which consumes $N \times N$ times multiple operation for each input vector. We want to speed up the caculation of $A'$. But we should notice:
 - Self-attention is noly a module in a larger network.
 - Self-attention dominates computation when $N$ is large. 
 - Usually developed for image processing. 

So speed up self-attention may not improve overall efficiency. 

The efficiency of different methods is shown as follows:
![](./figures/xxformer.png)

 - x-axis: Speed
 - y-axis: LRA Score
 - point size: Memory Consumption

### Local Attention
Count only nearby points and set other values ​​to zero.
![](./figures/local.png)

 - Fast
 - Similar to CNN
 - Low accurancy

### Stride Attention
Caculate weight fixed distance from origin. 
![](./figures/stride.png)

 - Can choose different stride

### Global Attention
Add special token into original sequence (such as start/end of sentence).
![](./figures/global.png)

 - Attend to every token $\rightarrow$ collect global information
 - Attended by every token $\rightarrow$ it knows global information

### Random Attention
Calculate attention weight randomly. 
![](figures/random.png)

### Longformer
![](./figures/longformer.png)

Combination of:
 - Local attention
 - Stride attention
 - Global attention

### Big Bird
![](./figures/bigbird.png)

Combination of:
 - Random attention
 - Local attention
 - Global attention

### Clustering
![](./figures/clustering.png)

**Reformer** and **Routing Transformer** use different method to cluster. 

### Learnable Patterns
A grid should be skipped or not is decided by another learned module.
![](./figures/learnable.png)

**Sinkhorn Sorting Network** use two steps to get final $A'$. Many input sequence share the same middle sequnence. 

### Linformer
Point to the attention matrix is a low rank matrix, which means there are many redundant columns. So we want to use a "thin" matrix to replace the attention matrix. 
![](./figures/linformer.png)

Compress the set of key and the set of value (don't compress the set of query). Reduce the number of queries may change output sequence length. 
![](./figures/lincompress.png)

Linformer compress the set of key and the set of value by multiple the set by a matrix which outputs $K$ linear combination of original input vectors. 

### Compressed Attention
Just like linformer, but use different way to compress the set of key and the set of value. 
![](./figures/convcompress.png)

Use CNN to reduce the number of keys and values. 

### Linear Transformer, Performer
Speed up computation by replacing $\exp(\boldsymbol{q}\cdot\boldsymbol{k})$ with $\phi(\boldsymbol{q})\cdot\phi(\boldsymbol{k})$. 

Then soft-max can be written as: 
$$
\boldsymbol{b}^k = \sum_i\frac{\exp(\boldsymbol{q}^k\cdot\boldsymbol{k}^i)}{\sum_j\exp(\boldsymbol{q}^k\cdot\boldsymbol{k}^j)}\boldsymbol{v}^i = \frac{\sum_i[\phi(\boldsymbol{q}^k)\cdot\phi(\boldsymbol{k}^i)]\boldsymbol{v}^i}{\sum_j\phi(\boldsymbol{q}^k)\cdot\phi(\boldsymbol{k}^j)} = \frac{V^TK\phi(\boldsymbol{q}^k)}{\boldsymbol{k}'\cdot\phi(\boldsymbol{q}^k)}
$$

Where $V_{ij} = \boldsymbol{v}^i_j, K_{ij} = \boldsymbol{k}^i_j, \boldsymbol{k}' = \sum_j\phi(\boldsymbol{k^j})$

We can use the same $VK^T, \boldsymbol{k}'$ over different $\boldsymbol{b}^k$ so we should only compute $VK^T, \boldsymbol{k}$ once. 

$$
V^TK =
 \begin{bmatrix}
\boldsymbol{v}^1 \cdots \boldsymbol{v}^N
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{k}^1 \\
\vdots \\
\boldsymbol{k}^N
\end{bmatrix} \\
\boldsymbol{k} = \sum_i\phi(\boldsymbol{k}^i)
$$

**Linear Transformer** and **Performer** use different $\phi(\boldsymbol{v})$ function. 

### Synthesizer
Treat $A'$ as a parameter of network. So $A'$ is regardless of input. 

Making the attention matrix irrelated to input doesn't reduce the performance of self-attention. 

### Attention-free?
Get rid of attention matrix and use other methods (such as fourier transforms, MLP) to replace that matrix. 